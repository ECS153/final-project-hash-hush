{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools \n",
    "import operator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "# https://stackoverflow.com/questions/4265988/generate-random-numbers-with-a-given-numerical-distribution\n",
    "# https://superuser.com/questions/513496/how-can-i-run-a-command-from-the-terminal-without-blocking-it\n",
    "# https://www.cyberciti.biz/faq/find-out-what-processes-are-running-in-the-background-on-linux/\n",
    "# password possible char\n",
    "# a-z    A-Z     1234567890      @%+\\/'!#$^?:,.(){}[]~-_*   26+26+10+24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit from https://songhuiming.github.io/pages/2017/08/20/build-recurrent-neural-network-from-scratch/\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "\n",
    "'''\n",
    " - model: \n",
    " - X_train:\n",
    " - y_train:\n",
    " - learning_rate:\n",
    " - nepoch:\n",
    " - evaluate loss_after:\n",
    "'''\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):\n",
    "    # keep track of the losses so that we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # for each training example...\n",
    "        print(str(epoch) + \"th epoch:\")\n",
    "        count = 0\n",
    "        for i in range(len(y_train)):\n",
    "            # one sgd step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "            count = count + 1\n",
    "            if(count % 1000000 == 0):\n",
    "                np.save('U.npy', model.U)\n",
    "                np.save('V.npy', model.V)\n",
    "                np.save('W.npy', model.W)\n",
    "                print(\"    \" + str(count))\n",
    "        \n",
    "        np.save('U.npy', model.U)\n",
    "        np.save('V.npy', model.V)\n",
    "        np.save('W.npy', model.W)\n",
    "        # optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: loss after num_examples_seen=%d epoch=%d: %f\" %(time, num_examples_seen, epoch, loss))\n",
    "            # adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"setting learning rate to %f\" %(learning_rate))\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4, continued = False):\n",
    "        # assign instance variable\n",
    "        self.word_dim = word_dim   # number of possible characters, in this case 86\n",
    "        self.hidden_dim = hidden_dim # number of hidden units \n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        if(continued):\n",
    "            self.U = np.load(\"U.npy\")\n",
    "            self.V = np.load(\"V.npy\")\n",
    "            self.W = np.load(\"W.npy\")\n",
    "        else:\n",
    "            # random initiate the parameters\n",
    "            self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "            self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "            self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def forward_progagation(self, x):\n",
    "        # total time steps / length of the password\n",
    "        T = len(x)\n",
    "\n",
    "        #intialize s, with the initial state s set to zero, we thus have T+1 rows, each with size of hidden_dim\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        #intialize o, \n",
    "        o = np.zeros((T, self.word_dim))\n",
    "\n",
    "        for t in np.arange(T):\n",
    "            # since x is a one-hot encoder, index U with x[t] is the same thing as mutiply U with tons of 0s\n",
    "            r = s[t-1]\n",
    "            print(r.shape)\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t])) \n",
    "        return [o, s]\n",
    "\n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_progagation(x)\n",
    "        return np.argmax(o, axis = 1)\n",
    "    \n",
    "    \n",
    "    # in this case x and y are 2-d arrays\n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # for each sentence ...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_progagation(x[i])\n",
    "            # we only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    # in this case x and y are 2-d arrays\n",
    "    def calculate_loss(self, x, y):\n",
    "        # divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x, y)/N\n",
    "    \n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # perform forward propagation\n",
    "        o, s = self.forward_progagation(x)\n",
    "        # we will accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1   # it is y_hat - y\n",
    "        # for each output backwards ...\n",
    "        for t in np.arange(T):\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)    # at time step t, shape is word_dim * hidden_dim\n",
    "            # initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            # given time step t, go back from time step t, to t-1, t-2, ...\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print(\"Backprogation step t=%d bptt step=%d\" %(t, bptt_step))\n",
    "                dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
    "                dLdU[:, x[bptt_step]] += delta_t\n",
    "                # update delta for next step\n",
    "                dleta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "            \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dLdU\n",
    "        self.V -= learning_rate * dLdV\n",
    "        self.W -= learning_rate * dLdW\n",
    "    \n",
    "    def generate_guess(self):\n",
    "        pw = []\n",
    "        s = np.zeros(self.hidden_dim)\n",
    "        o = np.zeros(self.word_dim)\n",
    "        char = 0\n",
    "        prob = 1\n",
    "        \n",
    "        for t in np.arange(100):\n",
    "            s = np.tanh(self.U[:, char] + self.W.dot(s))\n",
    "            o = softmax(self.V.dot(s))\n",
    "            \n",
    "            char = np.random.choice(np.arange(0, 97), p=o)\n",
    "            prob = 1 * o[char]\n",
    "            \n",
    "            if(char == 96 or char == 0):\n",
    "                pw = [x + 31 for x in pw] \n",
    "                s = ''.join(map(chr, pw)) \n",
    "                return s, prob\n",
    "            pw.append(char)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# around 70 secs for rockyou.txt\n",
    "# https://theasciicode.com.ar/ascii-control-characters/backspace-ascii-code-8.html\n",
    "# set range for char 32 - 126 in ascii,  95 chars, remove any pw cotains char outside the range\n",
    "# also we want an addtional char START and END indicates the start and end of the password, so totally 97 chars\n",
    "# END would only appear in y, START only in x \n",
    "\n",
    "def generate_training_pw(infile):\n",
    "    f = open(infile, \"r\", errors='ignore')\n",
    "    line = f.readline()\n",
    "    x = []\n",
    "    y = []\n",
    "    count = 0\n",
    "    while line != \"\":\n",
    "        l = [ord(i)-31 for i in line][:-1] # convert to int\n",
    "        if any(y > 95 or y < 1 for y in l):\n",
    "            line = f.readline()\n",
    "            continue\n",
    "        l.insert(0, 0)\n",
    "        x.append(l) # add to X\n",
    "\n",
    "        l.append(96)\n",
    "        l = l[1:]\n",
    "        y.append(l) # modify and add to y\n",
    "        line = f.readline()\n",
    "\n",
    "        #count\n",
    "        count = count+1\n",
    "        if(count % 1000000 == 0):\n",
    "            print(str(count / 1000000) + \"million\" , end = \"->\")\n",
    "    print(\"Finished with \" + str(count) + \"passwords\") \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0million->2.0million->3.0million->4.0million->5.0million->6.0million->7.0million->8.0million->9.0million->10.0million->11.0million->12.0million->13.0million->14.0million->Finished with 14330062passwords\n"
     ]
    }
   ],
   "source": [
    "x, y = generate_training_pw(\"rockyou.txt\") #input x,y\n",
    "\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch'\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n",
      "3400000\n",
      "3410000\n",
      "3420000\n",
      "3430000\n",
      "3440000\n",
      "3450000\n",
      "3460000\n",
      "3470000\n",
      "3480000\n",
      "3490000\n",
      "3500000\n",
      "3510000\n",
      "3520000\n",
      "3530000\n",
      "3540000\n",
      "3550000\n",
      "3560000\n",
      "3570000\n",
      "3580000\n",
      "3590000\n",
      "3600000\n",
      "3610000\n",
      "3620000\n",
      "3630000\n",
      "3640000\n",
      "3650000\n",
      "3660000\n",
      "3670000\n",
      "3680000\n",
      "3690000\n",
      "3700000\n",
      "3710000\n",
      "3720000\n",
      "3730000\n",
      "3740000\n",
      "3750000\n",
      "3760000\n",
      "3770000\n",
      "3780000\n",
      "3790000\n",
      "3800000\n",
      "3810000\n",
      "3820000\n",
      "3830000\n",
      "3840000\n",
      "3850000\n",
      "3860000\n",
      "3870000\n",
      "3880000\n",
      "3890000\n",
      "3900000\n",
      "3910000\n",
      "3920000\n",
      "3930000\n",
      "3940000\n",
      "3950000\n",
      "3960000\n",
      "3970000\n",
      "3980000\n",
      "3990000\n",
      "4000000\n",
      "4010000\n",
      "4020000\n",
      "4030000\n",
      "4040000\n",
      "4050000\n",
      "4060000\n",
      "4070000\n",
      "4080000\n",
      "4090000\n",
      "4100000\n",
      "4110000\n",
      "4120000\n",
      "4130000\n",
      "4140000\n",
      "4150000\n",
      "4160000\n",
      "4170000\n",
      "4180000\n",
      "4190000\n",
      "4200000\n",
      "4210000\n",
      "4220000\n",
      "4230000\n",
      "4240000\n",
      "4250000\n",
      "4260000\n",
      "4270000\n",
      "4280000\n",
      "4290000\n",
      "4300000\n",
      "4310000\n",
      "4320000\n",
      "4330000\n",
      "4340000\n",
      "4350000\n",
      "4360000\n",
      "4370000\n",
      "4380000\n",
      "4390000\n",
      "4400000\n",
      "4410000\n",
      "4420000\n",
      "4430000\n",
      "4440000\n",
      "4450000\n",
      "4460000\n",
      "4470000\n",
      "4480000\n",
      "4490000\n",
      "4500000\n",
      "4510000\n",
      "4520000\n",
      "4530000\n",
      "4540000\n",
      "4550000\n",
      "4560000\n",
      "4570000\n",
      "4580000\n",
      "4590000\n",
      "4600000\n",
      "4610000\n",
      "4620000\n",
      "4630000\n",
      "4640000\n",
      "4650000\n",
      "4660000\n",
      "4670000\n",
      "4680000\n",
      "4690000\n",
      "4700000\n",
      "4710000\n",
      "4720000\n",
      "4730000\n",
      "4740000\n",
      "4750000\n",
      "4760000\n",
      "4770000\n",
      "4780000\n",
      "4790000\n",
      "4800000\n",
      "4810000\n",
      "4820000\n",
      "4830000\n",
      "4840000\n",
      "4850000\n",
      "4860000\n",
      "4870000\n",
      "4880000\n",
      "4890000\n",
      "4900000\n",
      "4910000\n",
      "4920000\n",
      "4930000\n",
      "4940000\n",
      "4950000\n",
      "4960000\n",
      "4970000\n",
      "4980000\n",
      "4990000\n",
      "5000000\n",
      "5010000\n",
      "5020000\n",
      "5030000\n",
      "5040000\n",
      "5050000\n",
      "5060000\n",
      "5070000\n",
      "5080000\n",
      "5090000\n",
      "5100000\n",
      "5110000\n",
      "5120000\n",
      "5130000\n",
      "5140000\n",
      "5150000\n",
      "5160000\n",
      "5170000\n",
      "5180000\n",
      "5190000\n",
      "5200000\n",
      "5210000\n",
      "5220000\n",
      "5230000\n",
      "5240000\n",
      "5250000\n",
      "5260000\n",
      "5270000\n",
      "5280000\n",
      "5290000\n",
      "5300000\n",
      "5310000\n",
      "5320000\n",
      "5330000\n",
      "5340000\n",
      "5350000\n",
      "5360000\n",
      "5370000\n",
      "5380000\n",
      "5390000\n",
      "5400000\n",
      "5410000\n",
      "5420000\n",
      "5430000\n",
      "5440000\n",
      "5450000\n",
      "5460000\n",
      "5470000\n",
      "5480000\n",
      "5490000\n",
      "5500000\n",
      "5510000\n",
      "5520000\n",
      "5530000\n",
      "5540000\n",
      "5550000\n",
      "5560000\n",
      "5570000\n",
      "5580000\n",
      "5590000\n",
      "5600000\n",
      "5610000\n",
      "5620000\n",
      "5630000\n",
      "5640000\n",
      "5650000\n",
      "5660000\n",
      "5670000\n",
      "5680000\n",
      "5690000\n",
      "5700000\n",
      "5710000\n",
      "5720000\n",
      "5730000\n",
      "5740000\n",
      "5750000\n",
      "5760000\n",
      "5770000\n",
      "5780000\n",
      "5790000\n",
      "5800000\n",
      "5810000\n",
      "5820000\n",
      "5830000\n",
      "5840000\n",
      "5850000\n",
      "5860000\n",
      "5870000\n",
      "5880000\n",
      "5890000\n",
      "5900000\n",
      "5910000\n",
      "5920000\n",
      "5930000\n",
      "5940000\n",
      "5950000\n",
      "5960000\n",
      "5970000\n",
      "5980000\n",
      "5990000\n",
      "6000000\n",
      "6010000\n",
      "6020000\n",
      "6030000\n",
      "6040000\n",
      "6050000\n",
      "6060000\n",
      "6070000\n",
      "6080000\n",
      "6090000\n",
      "6100000\n",
      "6110000\n",
      "6120000\n",
      "6130000\n",
      "6140000\n",
      "6150000\n",
      "6160000\n",
      "6170000\n",
      "6180000\n",
      "6190000\n",
      "6200000\n",
      "6210000\n",
      "6220000\n",
      "6230000\n",
      "6240000\n",
      "6250000\n",
      "6260000\n",
      "6270000\n",
      "6280000\n",
      "6290000\n",
      "6300000\n",
      "6310000\n",
      "6320000\n",
      "6330000\n",
      "6340000\n",
      "6350000\n",
      "6360000\n",
      "6370000\n",
      "6380000\n",
      "6390000\n",
      "6400000\n",
      "6410000\n",
      "6420000\n",
      "6430000\n",
      "6440000\n",
      "6450000\n",
      "6460000\n",
      "6470000\n",
      "6480000\n",
      "6490000\n",
      "6500000\n",
      "6510000\n",
      "6520000\n",
      "6530000\n",
      "6540000\n",
      "6550000\n",
      "6560000\n",
      "6570000\n",
      "6580000\n",
      "6590000\n",
      "6600000\n",
      "6610000\n",
      "6620000\n",
      "6630000\n",
      "6640000\n",
      "6650000\n",
      "6660000\n",
      "6670000\n",
      "6680000\n",
      "6690000\n",
      "6700000\n",
      "6710000\n",
      "6720000\n",
      "6730000\n",
      "6740000\n",
      "6750000\n",
      "6760000\n",
      "6770000\n",
      "6780000\n",
      "6790000\n",
      "6800000\n",
      "6810000\n",
      "6820000\n",
      "6830000\n",
      "6840000\n",
      "6850000\n",
      "6860000\n",
      "6870000\n",
      "6880000\n",
      "6890000\n",
      "6900000\n",
      "6910000\n",
      "6920000\n",
      "6930000\n",
      "6940000\n",
      "6950000\n",
      "6960000\n",
      "6970000\n",
      "6980000\n",
      "6990000\n",
      "7000000\n",
      "7010000\n",
      "7020000\n",
      "7030000\n",
      "7040000\n",
      "7050000\n",
      "7060000\n",
      "7070000\n",
      "7080000\n",
      "7090000\n",
      "7100000\n",
      "7110000\n",
      "7120000\n",
      "7130000\n",
      "7140000\n",
      "7150000\n",
      "7160000\n",
      "7170000\n",
      "7180000\n",
      "7190000\n",
      "7200000\n",
      "7210000\n",
      "7220000\n",
      "7230000\n",
      "7240000\n",
      "7250000\n",
      "7260000\n",
      "7270000\n",
      "7280000\n",
      "7290000\n",
      "7300000\n",
      "7310000\n",
      "7320000\n",
      "7330000\n",
      "7340000\n",
      "7350000\n",
      "7360000\n",
      "7370000\n",
      "7380000\n",
      "7390000\n",
      "7400000\n",
      "7410000\n",
      "7420000\n",
      "7430000\n",
      "7440000\n",
      "7450000\n",
      "7460000\n",
      "7470000\n",
      "7480000\n",
      "7490000\n",
      "7500000\n",
      "7510000\n",
      "7520000\n",
      "7530000\n",
      "7540000\n",
      "7550000\n",
      "7560000\n",
      "7570000\n",
      "7580000\n",
      "7590000\n",
      "7600000\n",
      "7610000\n",
      "7620000\n",
      "7630000\n",
      "7640000\n",
      "7650000\n",
      "7660000\n",
      "7670000\n",
      "7680000\n",
      "7690000\n",
      "7700000\n",
      "7710000\n",
      "7720000\n",
      "7730000\n",
      "7740000\n",
      "7750000\n",
      "7760000\n",
      "7770000\n",
      "7780000\n",
      "7790000\n",
      "7800000\n",
      "7810000\n",
      "7820000\n",
      "7830000\n",
      "7840000\n",
      "7850000\n",
      "7860000\n",
      "7870000\n",
      "7880000\n",
      "7890000\n",
      "7900000\n",
      "7910000\n",
      "7920000\n",
      "7930000\n",
      "7940000\n",
      "7950000\n",
      "7960000\n",
      "7970000\n",
      "7980000\n",
      "7990000\n",
      "8000000\n",
      "8010000\n",
      "8020000\n",
      "8030000\n",
      "8040000\n",
      "8050000\n",
      "8060000\n",
      "8070000\n",
      "8080000\n",
      "8090000\n",
      "8100000\n",
      "8110000\n",
      "8120000\n",
      "8130000\n",
      "8140000\n",
      "8150000\n",
      "8160000\n",
      "8170000\n",
      "8180000\n",
      "8190000\n",
      "8200000\n",
      "8210000\n",
      "8220000\n",
      "8230000\n",
      "8240000\n",
      "8250000\n",
      "8260000\n",
      "8270000\n",
      "8280000\n",
      "8290000\n",
      "8300000\n",
      "8310000\n",
      "8320000\n",
      "8330000\n",
      "8340000\n",
      "8350000\n",
      "8360000\n",
      "8370000\n",
      "8380000\n",
      "8390000\n",
      "8400000\n",
      "8410000\n",
      "8420000\n",
      "8430000\n",
      "8440000\n",
      "8450000\n",
      "8460000\n",
      "8470000\n",
      "8480000\n",
      "8490000\n",
      "8500000\n",
      "8510000\n",
      "8520000\n",
      "8530000\n",
      "8540000\n",
      "8550000\n",
      "8560000\n",
      "8570000\n",
      "8580000\n",
      "8590000\n",
      "8600000\n",
      "8610000\n",
      "8620000\n",
      "8630000\n",
      "8640000\n",
      "8650000\n",
      "8660000\n",
      "8670000\n",
      "8680000\n",
      "8690000\n",
      "8700000\n",
      "8710000\n",
      "8720000\n",
      "8730000\n",
      "8740000\n",
      "8750000\n",
      "8760000\n",
      "8770000\n",
      "8780000\n",
      "8790000\n",
      "8800000\n",
      "8810000\n",
      "8820000\n",
      "8830000\n",
      "8840000\n",
      "8850000\n",
      "8860000\n",
      "8870000\n",
      "8880000\n",
      "8890000\n",
      "8900000\n",
      "8910000\n",
      "8920000\n",
      "8930000\n",
      "8940000\n",
      "8950000\n",
      "8960000\n",
      "8970000\n",
      "8980000\n",
      "8990000\n",
      "9000000\n",
      "9010000\n",
      "9020000\n",
      "9030000\n",
      "9040000\n",
      "9050000\n",
      "9060000\n",
      "9070000\n",
      "9080000\n",
      "9090000\n",
      "9100000\n",
      "9110000\n",
      "9120000\n",
      "9130000\n",
      "9140000\n",
      "9150000\n",
      "9160000\n",
      "9170000\n",
      "9180000\n",
      "9190000\n",
      "9200000\n",
      "9210000\n",
      "9220000\n",
      "9230000\n",
      "9240000\n",
      "9250000\n",
      "9260000\n",
      "9270000\n",
      "9280000\n",
      "9290000\n",
      "9300000\n",
      "9310000\n",
      "9320000\n",
      "9330000\n",
      "9340000\n",
      "9350000\n",
      "9360000\n",
      "9370000\n",
      "9380000\n",
      "9390000\n",
      "9400000\n",
      "9410000\n",
      "9420000\n",
      "9430000\n",
      "9440000\n",
      "9450000\n",
      "9460000\n",
      "9470000\n",
      "9480000\n",
      "9490000\n",
      "9500000\n",
      "9510000\n",
      "9520000\n",
      "9530000\n",
      "9540000\n",
      "9550000\n",
      "9560000\n",
      "9570000\n",
      "9580000\n",
      "9590000\n",
      "9600000\n",
      "9610000\n",
      "9620000\n",
      "9630000\n",
      "9640000\n",
      "9650000\n",
      "9660000\n",
      "9670000\n",
      "9680000\n",
      "9690000\n",
      "9700000\n",
      "9710000\n",
      "9720000\n",
      "9730000\n",
      "9740000\n",
      "9750000\n",
      "9760000\n",
      "9770000\n",
      "9780000\n",
      "9790000\n",
      "9800000\n",
      "9810000\n",
      "9820000\n",
      "9830000\n",
      "9840000\n",
      "9850000\n",
      "9860000\n",
      "9870000\n",
      "9880000\n",
      "9890000\n",
      "9900000\n",
      "9910000\n",
      "9920000\n",
      "9930000\n",
      "9940000\n",
      "9950000\n",
      "9960000\n",
      "9970000\n",
      "9980000\n",
      "9990000\n",
      "10000000\n",
      "10010000\n",
      "10020000\n",
      "10030000\n",
      "10040000\n",
      "10050000\n",
      "10060000\n",
      "10070000\n",
      "10080000\n",
      "10090000\n",
      "10100000\n",
      "10110000\n",
      "10120000\n",
      "10130000\n",
      "10140000\n",
      "10150000\n",
      "10160000\n",
      "10170000\n",
      "10180000\n",
      "10190000\n",
      "10200000\n",
      "10210000\n",
      "10220000\n",
      "10230000\n",
      "10240000\n",
      "10250000\n",
      "10260000\n",
      "10270000\n",
      "10280000\n",
      "10290000\n",
      "10300000\n",
      "10310000\n",
      "10320000\n",
      "10330000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10340000\n",
      "10350000\n",
      "10360000\n",
      "10370000\n",
      "10380000\n",
      "10390000\n",
      "10400000\n",
      "10410000\n",
      "10420000\n",
      "10430000\n",
      "10440000\n",
      "10450000\n",
      "10460000\n",
      "10470000\n",
      "10480000\n",
      "10490000\n",
      "10500000\n",
      "10510000\n",
      "10520000\n",
      "10530000\n",
      "10540000\n",
      "10550000\n",
      "10560000\n",
      "10570000\n",
      "10580000\n",
      "10590000\n",
      "10600000\n",
      "10610000\n",
      "10620000\n",
      "10630000\n",
      "10640000\n",
      "10650000\n",
      "10660000\n",
      "10670000\n",
      "10680000\n",
      "10690000\n",
      "10700000\n",
      "10710000\n",
      "10720000\n",
      "10730000\n",
      "10740000\n",
      "10750000\n",
      "10760000\n",
      "10770000\n",
      "10780000\n",
      "10790000\n",
      "10800000\n",
      "10810000\n",
      "10820000\n",
      "10830000\n",
      "10840000\n",
      "10850000\n",
      "10860000\n",
      "10870000\n",
      "10880000\n",
      "10890000\n",
      "10900000\n",
      "10910000\n",
      "10920000\n",
      "10930000\n",
      "10940000\n",
      "10950000\n",
      "10960000\n",
      "10970000\n",
      "10980000\n",
      "10990000\n",
      "11000000\n",
      "11010000\n",
      "11020000\n",
      "11030000\n",
      "11040000\n",
      "11050000\n",
      "11060000\n",
      "11070000\n",
      "11080000\n",
      "11090000\n",
      "11100000\n",
      "11110000\n",
      "11120000\n",
      "11130000\n",
      "11140000\n",
      "11150000\n",
      "11160000\n",
      "11170000\n",
      "11180000\n",
      "11190000\n",
      "11200000\n",
      "11210000\n",
      "11220000\n",
      "11230000\n",
      "11240000\n",
      "11250000\n",
      "11260000\n",
      "11270000\n",
      "11280000\n",
      "11290000\n",
      "11300000\n",
      "11310000\n",
      "11320000\n",
      "11330000\n",
      "11340000\n",
      "11350000\n",
      "11360000\n",
      "11370000\n",
      "11380000\n",
      "11390000\n",
      "11400000\n",
      "11410000\n",
      "11420000\n",
      "11430000\n",
      "11440000\n",
      "11450000\n",
      "11460000\n",
      "11470000\n",
      "11480000\n",
      "11490000\n",
      "11500000\n",
      "11510000\n",
      "11520000\n",
      "11530000\n",
      "11540000\n",
      "11550000\n",
      "11560000\n",
      "11570000\n",
      "11580000\n",
      "11590000\n",
      "11600000\n",
      "11610000\n",
      "11620000\n",
      "11630000\n",
      "11640000\n",
      "11650000\n",
      "11660000\n",
      "11670000\n",
      "11680000\n",
      "11690000\n",
      "11700000\n",
      "11710000\n",
      "11720000\n",
      "11730000\n",
      "11740000\n",
      "11750000\n",
      "11760000\n",
      "11770000\n",
      "11780000\n",
      "11790000\n",
      "11800000\n",
      "11810000\n",
      "11820000\n",
      "11830000\n",
      "11840000\n",
      "11850000\n",
      "11860000\n",
      "11870000\n",
      "11880000\n",
      "11890000\n",
      "11900000\n",
      "11910000\n",
      "11920000\n",
      "11930000\n",
      "11940000\n",
      "11950000\n",
      "11960000\n",
      "11970000\n",
      "11980000\n",
      "11990000\n",
      "12000000\n",
      "12010000\n",
      "12020000\n",
      "12030000\n",
      "12040000\n",
      "12050000\n",
      "12060000\n",
      "12070000\n",
      "12080000\n",
      "12090000\n",
      "12100000\n",
      "12110000\n",
      "12120000\n",
      "12130000\n",
      "12140000\n",
      "12150000\n",
      "12160000\n",
      "12170000\n",
      "12180000\n",
      "12190000\n",
      "12200000\n",
      "12210000\n",
      "12220000\n",
      "12230000\n",
      "12240000\n",
      "12250000\n",
      "12260000\n",
      "12270000\n",
      "12280000\n",
      "12290000\n",
      "12300000\n",
      "12310000\n",
      "12320000\n",
      "12330000\n",
      "12340000\n",
      "12350000\n",
      "12360000\n",
      "12370000\n",
      "12380000\n",
      "12390000\n",
      "12400000\n",
      "12410000\n",
      "12420000\n",
      "12430000\n",
      "12440000\n",
      "12450000\n",
      "12460000\n",
      "12470000\n",
      "12480000\n",
      "12490000\n",
      "12500000\n",
      "12510000\n",
      "12520000\n",
      "12530000\n",
      "12540000\n",
      "12550000\n",
      "12560000\n",
      "12570000\n",
      "12580000\n",
      "12590000\n",
      "12600000\n",
      "12610000\n",
      "12620000\n",
      "12630000\n",
      "12640000\n",
      "12650000\n",
      "12660000\n",
      "12670000\n",
      "12680000\n",
      "12690000\n",
      "12700000\n",
      "12710000\n",
      "12720000\n",
      "12730000\n",
      "12740000\n",
      "12750000\n",
      "12760000\n",
      "12770000\n",
      "12780000\n",
      "12790000\n",
      "12800000\n",
      "12810000\n",
      "12820000\n",
      "12830000\n",
      "12840000\n",
      "12850000\n",
      "12860000\n",
      "12870000\n",
      "12880000\n",
      "12890000\n",
      "12900000\n",
      "12910000\n",
      "12920000\n",
      "12930000\n",
      "12940000\n",
      "12950000\n",
      "12960000\n",
      "12970000\n",
      "12980000\n",
      "12990000\n",
      "13000000\n",
      "13010000\n",
      "13020000\n",
      "13030000\n",
      "13040000\n",
      "13050000\n",
      "13060000\n",
      "13070000\n",
      "13080000\n",
      "13090000\n",
      "13100000\n",
      "13110000\n",
      "13120000\n",
      "13130000\n",
      "13140000\n",
      "13150000\n",
      "13160000\n",
      "13170000\n",
      "13180000\n",
      "13190000\n",
      "13200000\n",
      "13210000\n",
      "13220000\n",
      "13230000\n",
      "13240000\n",
      "13250000\n",
      "13260000\n",
      "13270000\n",
      "13280000\n",
      "13290000\n",
      "13300000\n",
      "13310000\n",
      "13320000\n",
      "13330000\n",
      "13340000\n",
      "13350000\n",
      "13360000\n",
      "13370000\n",
      "13380000\n",
      "13390000\n",
      "13400000\n",
      "13410000\n",
      "13420000\n",
      "13430000\n",
      "13440000\n",
      "13450000\n",
      "13460000\n",
      "13470000\n",
      "13480000\n",
      "13490000\n",
      "13500000\n",
      "13510000\n",
      "13520000\n",
      "13530000\n",
      "13540000\n",
      "13550000\n",
      "13560000\n",
      "13570000\n",
      "13580000\n",
      "13590000\n",
      "13600000\n",
      "13610000\n",
      "13620000\n",
      "13630000\n",
      "13640000\n",
      "13650000\n",
      "13660000\n",
      "13670000\n",
      "13680000\n",
      "13690000\n",
      "13700000\n",
      "13710000\n",
      "13720000\n",
      "13730000\n",
      "13740000\n",
      "13750000\n",
      "13760000\n",
      "13770000\n",
      "13780000\n",
      "13790000\n",
      "13800000\n",
      "13810000\n",
      "13820000\n",
      "13830000\n",
      "13840000\n",
      "13850000\n",
      "13860000\n",
      "13870000\n",
      "13880000\n",
      "13890000\n",
      "13900000\n",
      "13910000\n",
      "13920000\n",
      "13930000\n",
      "13940000\n",
      "13950000\n",
      "13960000\n",
      "13970000\n",
      "13980000\n",
      "13990000\n",
      "14000000\n",
      "14010000\n",
      "14020000\n",
      "14030000\n",
      "14040000\n",
      "14050000\n",
      "14060000\n",
      "14070000\n",
      "14080000\n",
      "14090000\n",
      "14100000\n",
      "14110000\n",
      "14120000\n",
      "14130000\n",
      "14140000\n",
      "14150000\n",
      "14160000\n",
      "14170000\n",
      "14180000\n",
      "14190000\n",
      "14200000\n",
      "14210000\n",
      "14220000\n",
      "14230000\n",
      "14240000\n",
      "14250000\n",
      "14260000\n",
      "14270000\n",
      "14280000\n",
      "14290000\n",
      "14300000\n",
      "14310000\n",
      "14320000\n",
      "14330000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-19 14:32:42: loss after num_examples_seen=14330062 epoch=0: 7.821211\n",
      "1th epoch'\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-ba5c14b73135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_loss_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-eb8667c64fd8>\u001b[0m in \u001b[0;36mtrain_with_sgd\u001b[0;34m(model, X_train, y_train, learning_rate, nepoch, evaluate_loss_after)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# one sgd step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mnum_examples_seen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3cfac304a0f9>\u001b[0m in \u001b[0;36msgd_step\u001b[0;34m(self, x, y, learning_rate)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mdLdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3cfac304a0f9>\u001b[0m in \u001b[0;36mbptt\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mdLdU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# update delta for next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mdleta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdLdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdW\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(10)\n",
    "char_size = 97\n",
    "\n",
    "if(offset == 0):\n",
    "    model = RNNNumpy(char_size)\n",
    "else:\n",
    "    # Insert previous U, V, W into model \n",
    "    model = RNNNumpy(char_size, continued=True)\n",
    "    \n",
    "    #finsh up this epoch\n",
    "    losses = train_with_sgd(model, x, y, nepoch = 1)\n",
    "\n",
    "losses = train_with_sgd(model, x, y, nepoch = 10, evaluate_loss_after = 5)\n",
    "\n",
    "\n",
    "# TODO: Generate random passwords, then check for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "48\n",
      "82\n",
      "64\n",
      "58\n",
      "2\n",
      "13\n",
      "34\n",
      "90\n",
      "69\n",
      "8\n",
      "45\n",
      "79\n",
      "10\n",
      "25\n",
      "42\n",
      "50\n",
      "58\n",
      "66\n",
      "26\n",
      "29\n",
      "78\n",
      "30\n",
      "95\n",
      "88\n",
      "75\n",
      "66\n",
      "57\n",
      "7\n",
      "65\n",
      "16\n",
      "50\n",
      "78\n",
      "57\n",
      "33\n",
      "1\n",
      "14\n",
      "50\n",
      "49\n",
      "87\n",
      "6\n",
      "91\n",
      "6\n",
      "3\n",
      "67\n",
      "68\n",
      "70\n",
      "30\n",
      "82\n",
      "12\n",
      "44\n",
      "37\n",
      "14\n",
      "27\n",
      "14\n",
      "59\n",
      "56\n",
      "15\n",
      "15\n",
      "70\n",
      "92\n",
      "16\n",
      "93\n",
      "31\n",
      "26\n",
      "41\n",
      "29\n",
      "78\n",
      "80\n",
      "76\n",
      "85\n",
      "48\n",
      "59\n",
      "61\n",
      "38\n",
      "16\n",
      "75\n",
      "28\n",
      "4\n",
      "86\n",
      "49\n",
      "95\n",
      "41\n",
      "41\n",
      "95\n",
      "39\n",
      "88\n",
      "89\n",
      "29\n",
      "35\n",
      "48\n",
      "13\n",
      "29\n",
      "15\n",
      "46\n",
      "52\n",
      "57\n",
      "64\n",
      "83\n",
      "50\n",
      "53\n",
      "38\n",
      "38\n",
      "51\n",
      "63\n",
      "94\n",
      "87\n",
      "9\n",
      "76\n",
      "12\n",
      "17\n",
      "52\n",
      "36\n",
      "12\n",
      "71\n",
      "32\n",
      "60\n",
      "88\n",
      "11\n",
      "3\n",
      "9\n",
      "58\n",
      "70\n",
      "11\n",
      "14\n",
      "24\n",
      "27\n",
      "2\n",
      "54\n",
      "25\n",
      "25\n",
      "62\n",
      "18\n",
      "35\n",
      "16\n",
      "76\n",
      "40\n",
      "48\n",
      "21\n",
      "87\n",
      "75\n",
      "39\n",
      "78\n",
      "8\n",
      "33\n",
      "78\n",
      "60\n",
      "10\n",
      "24\n",
      "86\n",
      "60\n",
      "70\n",
      "89\n",
      "61\n",
      "12\n",
      "72\n",
      "50\n",
      "11\n",
      "63\n",
      "9\n",
      "17\n",
      "72\n",
      "94\n",
      "27\n",
      "80\n",
      "21\n",
      "67\n",
      "73\n",
      "67\n",
      "41\n",
      "41\n",
      "80\n",
      "59\n",
      "39\n",
      "95\n",
      "56\n",
      "57\n",
      "85\n",
      "6\n",
      "19\n",
      "44\n",
      "77\n",
      "46\n",
      "90\n",
      "44\n",
      "39\n",
      "55\n",
      "14\n",
      "49\n",
      "53\n",
      "52\n",
      "53\n",
      "5\n",
      "14\n",
      "56\n",
      "5\n",
      "5\n",
      "44\n",
      "27\n",
      "43\n",
      "10\n",
      "95\n",
      "83\n",
      "19\n",
      "38\n",
      "86\n",
      "9\n",
      "14\n",
      "70\n",
      "20\n",
      "87\n",
      "49\n",
      "2\n",
      "19\n",
      "76\n",
      "43\n",
      "60\n",
      "43\n",
      "73\n",
      "60\n",
      "36\n",
      "19\n",
      "41\n",
      "83\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-07cbbe318b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_guess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "file = open('myfile.txt', 'w') \n",
    "char_size = 97\n",
    "\n",
    "model = RNNNumpy(char_size, continued=True)\n",
    "i = 0\n",
    "count = 0\n",
    "while i < 10:\n",
    "    i = i + 1\n",
    "    s, prob = model.generate_guess()\n",
    "    if(prob > 0.2):\n",
    "        count = count + 1\n",
    "        file.writelines(s + '\\n')\n",
    "        \n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'aaa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-0ae3e26c9a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aaa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'aaa'"
     ]
    }
   ],
   "source": [
    "float(\"aaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 65, 66, 1, 32, 0, 17, 18, 66, 66, 67, 67]\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(char_size, continued=True)\n",
    "s = \"aab!@ 12bbcc\"\n",
    "l = []\n",
    "for i in s:\n",
    "    l.append(ord(i)-32)\n",
    "print(l)\n",
    "\n",
    "o, s = model.forward_progagation(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-91-4d0beae01f19>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-91-4d0beae01f19>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# possibily not going to use this  \n",
    "    def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):\n",
    "        # calculate the gradient using backpropagation\n",
    "        bptt_gradients = self.bptt(x, y)\n",
    "        # list of all params we want to check\n",
    "        model_parameters = [\"U\", \"V\", \"W\"]\n",
    "        # gradient check for each parameter\n",
    "        for pidx, pname in enumerate(model_parameters):\n",
    "            # get the actual parameter value from model, e.g. model.W\n",
    "            parameter = operator.attrgetter(pname)(self)\n",
    "            print(\"performing gradient check for parameter %s with size %d. \" %(pname, np.prod(parameter.shape)))\n",
    "            # iterate over each element of the parameter matrix, e.g. (0,0), (0,1)...\n",
    "            it = np.nditer(parameter, flags = ['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                # save the original value so we can reset it later\n",
    "                original_value = parameter[ix]\n",
    "                # estimate the gradient using (f(x+h) - f(x-h))/2h\n",
    "                parameter[ix] = original_value + h\n",
    "                gradplus = self.calculate_total_loss([x], [y])\n",
    "                parameter[ix] = original_value - h\n",
    "                gradminus = self.calculate_total_loss([x], [y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                # reset parameter to the original value\n",
    "                parameter[ix] = original_value\n",
    "                # the gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "                # calculate the relative error (|x - y|)/(|x|+|y|)\n",
    "                relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "                # if the error is too large fail the gradient check\n",
    "                if relative_error < error_threshold:\n",
    "                    print(\"Gradient check error: parameter = %s ix = %s\" %(pname, ix))\n",
    "                    print(\"+h Loss: %f\" % gradplus)\n",
    "                    print(\"-h Loss: %f\" % gradminus)\n",
    "                    print(\"Estimated gradient: %f\" % estimated_gradient)\n",
    "                    print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                    print(\"Relative error: %f\" % relative_error)\n",
    "                    return\n",
    "                it.iternext()\n",
    "            print(\"Gradient check for parameter %s passed. \" %(pname))\n",
    "            \n",
    "print(\"Expected Loss for random prediction: %f\" % np.log(char_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(x, y))\n",
    "\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate = 1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
